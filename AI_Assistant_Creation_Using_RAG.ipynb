{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook Adaptado para Exercício RAG - Fontes Web/PDFs EXPANDIDAS (~110 fontes, Histórico 1990s-Presente).\n",
        "Usando FAISS (ADA-002) para persistência local. SEM VÍDEOS YOUTUBE. ALTA CARGA DE RECURSOS.\n"
      ],
      "metadata": {
        "id": "7woAtqW5QQYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 1. Setup (Install and Import) ==="
      ],
      "metadata": {
        "id": "9PEcIl10P1S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Instalando/Atualizando bibliotecas necessárias (incluindo faiss-cpu)...\")\n",
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph langchain-openai langchain-core pypdf unstructured requests tiktoken faiss-cpu\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "import requests\n",
        "import tempfile\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain import hub\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "from langgraph.graph import START, StateGraph\n",
        "from IPython.display import Image, display\n",
        "import time\n",
        "import shutil\n",
        "import traceback\n",
        "from urllib.parse import urlparse # To help clean source names\n",
        "\n",
        "print(\"Importações concluídas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5CS9sQ0P3Um",
        "outputId": "41195c6c-0213-45cb-8776-af418eb6a429"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando/Atualizando bibliotecas necessárias (incluindo faiss-cpu)...\n",
            "Importações concluídas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurando a chave da API\n",
        "try:\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if not openai_api_key: raise ValueError(\"API Key not found or empty.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    print(\"Chave da API OpenAI carregada do Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao buscar chave do Colab Secrets ({e}). Solicitando input.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Cole sua Chave da API OpenAI aqui: \")\n",
        "    if not os.environ[\"OPENAI_API_KEY\"]: print(\"ERRO: Nenhuma chave da API fornecida. Saindo.\"); exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OSlUq50P3MD",
        "outputId": "2b8442f5-b010-4a75-9766-d9dd78b04746"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chave da API OpenAI carregada do Colab Secrets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializando o LLM\n",
        "LLM_MODEL_NAME = \"gpt-4o-mini\" # Keep mini for cost during large indexing potentially\n",
        "try:\n",
        "    llm = ChatOpenAI(model=LLM_MODEL_NAME, temperature=0, request_timeout=180) # Even longer timeout\n",
        "    print(f\"Modelo LLM ({llm.model_name}) inicializado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro crítico ao inicializar o LLM ({LLM_MODEL_NAME}): {e}\"); print(traceback.format_exc()[:1000]); exit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfRg8VrdP3ES",
        "outputId": "c3a57347-8936-4bb8-b771-2bab35d395ce"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo LLM (gpt-4o-mini) inicializado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 2. Definir Fontes de Dados EXPANDIDAS (~110 Fontes: Blogs/PDFs, ~1990s - Presente) ===\n",
        "\n",
        "- COMBINAÇÃO DAS LISTAS ANTERIORES + NOVAS, REMOVENDO DUPLICATAS"
      ],
      "metadata": {
        "id": "OW2mBHWwRWn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Definindo Fontes de Dados EXPANDIDAS (~110 Selecionadas: Blogs/PDFs Históricos e Recentes) ---\")\n",
        "\n",
        "# Use sets for easy deduplication\n",
        "unique_web_urls = set()\n",
        "unique_pdf_urls = set()\n",
        "\n",
        "# --- Web Pages / Blog Posts / Articles ---\n",
        "initial_web_page_urls = [\n",
        "    # Recentes (2023-2024 foco)\n",
        "    \"https://openai.com/index/hello-gpt-4o/\", \"https://ai.meta.com/blog/meta-llama-3/\",\n",
        "    \"https://blog.google/technology/ai/google-io-2024-ai-announcements/\",\n",
        "    \"https://blogs.microsoft.com/ai/introducing-phi-3-redefining-whats-possible-with-slms/\",\n",
        "    \"https://huggingface.co/blog/llama3\", \"https://www.wired.com/story/google-io-2024-biggest-ai-announcements/\",\n",
        "    \"https://www.theverge.com/2024/5/13/24155443/openai-gpt-4o-launch-event-live-blog\", \"https://openai.com/blog/chatgpt/\",\n",
        "    \"https://www.technologyreview.com/2024/01/08/1086140/10-breakthrough-technologies-2024/\",\n",
        "    \"https://www.wired.com/story/what-is-generative-ai/\", \"https://www.nature.com/articles/d41586-024-01449-z\", # Nature AlphaFold 3\n",
        "\n",
        "    # Históricos e Conceituais (Pré-2023 + Explainers)\n",
        "    \"http://yann.lecun.com/exdb/lenet/\", \"https://jalammar.github.io/illustrated-transformer/\",\n",
        "    \"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\", \"https://distill.pub/2016/augmented-rnns/\",\n",
        "    \"https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\",\n",
        "    \"https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\",\n",
        "    \"https://openai.com/research/dall-e\", \"https://deepmind.google/discover/blog/alphago-the-story-so-far/\",\n",
        "    \"https://deepmind.google/discover/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology/\",\n",
        "    \"https://spectrum.ieee.org/deep-learning\", \"https://www.kdnuggets.com/2017/04/simple-introduction-support-vector-machines.html\",\n",
        "    \"https://web.stanford.edu/~hastie/ElemStatLearn/\", \"https://builtin.com/data-science/reinforcement-learning\",\n",
        "    \"https://en.wikipedia.org/wiki/AI_winter\", \"https://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence\",\n",
        "    \"https://ai.stanford.edu/\", \"https://aiindex.stanford.edu/\",\n",
        "\n",
        "    # Novas adições Web/Blog\n",
        "    \"https://www.nature.com/articles/nature14539\", # Deep Learning Review (LeCun, Bengio, Hinton) Nature Page\n",
        "    \"https://blog.google/technology/ai/introducing-gemini-our-largest-and-most-capable-ai-model/\", # Gemini Blog\n",
        "    \"https://ai.meta.com/blog/segment-anything-foundation-model-image-segmentation/\", # SAM Blog\n",
        "    \"https://pytorch.org/blog/pytorch-2-release/\", # PyTorch 2.0 Blog\n",
        "    \"https://timdettmers.com/2023/01/30/vicuna-the-missing-link-between-llm-and-chatgpt/\", # Vicuna Blog Post (Example)\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\", # Lilian Weng AI Agents\n",
        "    \"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\", # Lilian Weng Diffusion Models\n",
        "    \"https://sebastianraschka.com/blog/2023/llm-finetuning-explained.html\", # Raschka Finetuning Explainer\n",
        "    \"https://huggingface.co/blog/stable_diffusion\", # HF Stable Diffusion Blog\n",
        "    \"https://www.deeplearning.ai/the-batch/\", # The Batch Newsletter (Link to archive/main page)\n",
        "    \"https://developer.nvidia.com/blog/tag/generative-ai/\", # Nvidia Generative AI Tag\n",
        "    \"https://research.facebook.com/blog/2017/01/fair-open-sources-fasttext/\", # Facebook fastText Blog\n",
        "    \"https://www.explainxkcd.com/wiki/index.php/2860:_Unintended_Uses\", # XKCD on Alignment (Context)\n",
        "    \"https://futureoflife.org/ai-safety-research/\", # FLI AI Safety Page\n",
        "    \"https://www.eff.org/ai/resources\", # EFF AI Resources Page\n",
        "    \"https://en.wikipedia.org/wiki/Convolutional_neural_network\", # Wikipedia CNN\n",
        "    \"https://en.wikipedia.org/wiki/Recurrent_neural_network\", # Wikipedia RNN\n",
        "    \"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\", # TDS CNN\n",
        "    \"https://towardsdatascience.com/introduction-to-reinforcement-learning-rl-part-1-e1d95a6c96ac\", # TDS RL Intro\n",
        "    \"https://cset.georgetown.edu/publication/understanding-ai-ethics-and-safety-a-guide-for-the-conscientious-objector/\", # CSET Ethics Example\n",
        "    \"https://aws.amazon.com/what-is/vector-database/\", # AWS Vector DB Explainer\n",
        "    \"https://openai.com/policies/usage-policies\", # OpenAI Usage Policies\n",
        "    \"https://www.assemblyai.com/blog/diffusion-models-for-visual-data-a-deep-dive/\", # AssemblyAI Diffusion Blog\n",
        "    \"https://magazine.towardsdatascience.com/attention-is-all-you-need-the-neural-network-that-revolutionised-nlp-60ab88d3e85f\", # TDS Transformer/Attention\n",
        "    \"https://developer.ibm.com/articles/intro-computational-linguistics/\", # IBM Computational Linguistics Intro\n",
        "    \"https://huyenchip.com/blog/\", # Chip Huyen's Blog (Link main page)\n",
        "    \"https://blog.research.google/\", # Google Research Blog (Main)\n",
        "    \"https://github.blog/category/copilot/\", # GitHub Copilot Blog Category\n",
        "    \"https://aws.amazon.com/blogs/machine-learning/\", # AWS ML Blog (Main)\n",
        "]\n",
        "\n",
        "# --- PDFs (arXiv / Reports) ---\n",
        "initial_pdf_urls = [\n",
        "    # Recentes (2023-2024 Foco)\n",
        "    \"https://arxiv.org/pdf/2404.11225\", \"https://arxiv.org/pdf/2404.14219\",\n",
        "    \"https://cdn.openai.com/gpt-4o-system-card.pdf\", \"https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf\",\n",
        "    \"https://arxiv.org/pdf/2305.18290.pdf\", \"https://arxiv.org/pdf/2303.10130.pdf\",\n",
        "\n",
        "    # Históricos (Pré-2023 - Deep Learning Era etc.)\n",
        "    \"https://arxiv.org/pdf/1301.3781.pdf\", \"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\", # AlexNet\n",
        "    \"https://arxiv.org/pdf/1706.03762.pdf\", \"https://arxiv.org/pdf/1810.04805.pdf\", # BERT\n",
        "    \"https://arxiv.org/pdf/1406.2661.pdf\", \"https://arxiv.org/pdf/2005.14165.pdf\", # GPT-3\n",
        "    \"https://arxiv.org/pdf/1409.3215.pdf\", \"https://arxiv.org/pdf/1409.0473.pdf\", # Bahdanau Attention\n",
        "    \"https://arxiv.org/pdf/2103.00020.pdf\", \"https://arxiv.org/pdf/2006.11239.pdf\", # DDPM\n",
        "    \"https://arxiv.org/pdf/2203.02155.pdf\", \"https://arxiv.org/pdf/2205.11487.pdf\", # Chain-of-Thought\n",
        "    \"https://arxiv.org/pdf/1511.06434.pdf\", \"https://arxiv.org/pdf/1506.02640.pdf\", # Faster R-CNN\n",
        "    \"https://arxiv.org/pdf/1312.6114.pdf\", \"https://arxiv.org/pdf/1603.05027.pdf\", # WaveNet\n",
        "    \"https://arxiv.org/pdf/1703.06870.pdf\", # SAGAN\n",
        "\n",
        "    # Novas adições PDF\n",
        "    \"https://arxiv.org/pdf/1512.03385.pdf\", # ResNet\n",
        "    \"https://arxiv.org/pdf/1412.6980.pdf\", # Adam Optimizer\n",
        "    \"https://arxiv.org/pdf/1502.01852.pdf\", # Batch Normalization\n",
        "    \"https://arxiv.org/pdf/1710.10903.pdf\", # StyleGAN\n",
        "    \"https://arxiv.org/pdf/1911.11427.pdf\", # BART\n",
        "    \"https://arxiv.org/pdf/1907.11692.pdf\", # RoBERTa\n",
        "    \"https://arxiv.org/pdf/2010.11929.pdf\", # Vision Transformer (ViT)\n",
        "    \"https://arxiv.org/pdf/2106.09685.pdf\", # DreamerV2 (RL)\n",
        "    \"https://arxiv.org/pdf/2104.09755.pdf\", # FLAN\n",
        "    \"https://arxiv.org/pdf/2112.10752.pdf\", # GLIDE (Diffusion)\n",
        "    \"https://arxiv.org/pdf/2204.02311.pdf\", # LaMDA\n",
        "    # RLHF Survey needed better link, potentially find openreview or blog summary\n",
        "    \"https://arxiv.org/pdf/2304.08485.pdf\", # Self-Instruct\n",
        "    \"https://arxiv.org/pdf/2302.13971.pdf\", # LLaMA (Original)\n",
        "    \"https://arxiv.org/pdf/2306.15595.pdf\", # Segment Anything (SAM)\n",
        "    \"https://arxiv.org/pdf/2305.11206.pdf\", # Voyager (Agents)\n",
        "    \"https://arxiv.org/pdf/2308.10383.pdf\", # Llama 2\n",
        "    \"https://arxiv.org/pdf/2307.09288.pdf\", # Toolformer\n",
        "    \"https://arxiv.org/pdf/2401.04088.pdf\", # Mixtral of Experts\n",
        "    \"https://arxiv.org/pdf/2402.17764.pdf\", # Gemma Paper\n",
        "    \"https://arxiv.org/pdf/2304.07193.pdf\", # AlphaDev\n",
        "    \"https://arxiv.org/pdf/2310.13592.pdf\", # Zephyr\n",
        "    \"https://arxiv.org/pdf/cs/9704103.pdf\", # Boosting (Schapire) - Check accessibility\n",
        "    \"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\", # TensorFlow Whitepaper\n",
        "    \"https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\", # Glorot Initialization\n",
        "    \"https://arxiv.org/pdf/1707.06347.pdf\", # PPO (RL)\n",
        "    \"https://arxiv.org/pdf/1909.11553.pdf\", # T5\n",
        "    \"https://arxiv.org/pdf/2311.10122.pdf\", # GNN Survey\n",
        "    \"https://arxiv.org/pdf/2306.06174.pdf\", # Formal Algorithms for Transformers\n",
        "    # Find AI Index 2023/2024 PDFs if possible, e.g. searching \"Stanford HAI AI Index 2024 PDF\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTUfZYNMP3AO",
        "outputId": "d3015d44-c717-43a1-ed76-377762a7a097"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Definindo Fontes de Dados EXPANDIDAS (~110 Selecionadas: Blogs/PDFs Históricos e Recentes) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deduplicate and finalize lists\n",
        "web_page_urls = sorted(list(set(initial_web_page_urls)))\n",
        "pdf_urls = sorted(list(set(initial_pdf_urls)))\n",
        "\n",
        "final_total_sources = len(web_page_urls) + len(pdf_urls)\n",
        "print(f\"\\nFinal Unique Web Sources Defined: {len(web_page_urls)}\")\n",
        "print(f\"Final Unique PDF Sources Defined: {len(pdf_urls)}\")\n",
        "print(f\"FINAL GRAND TOTAL Unique Sources: {final_total_sources}\")\n",
        "print(\"------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3bVvUDDP28D",
        "outputId": "1877f323-9b2c-482c-b7e6-f0d357c28c7a"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Unique Web Sources Defined: 57\n",
            "Final Unique PDF Sources Defined: 51\n",
            "FINAL GRAND TOTAL Unique Sources: 108\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 3. Carregar Dados ==="
      ],
      "metadata": {
        "id": "OEOFq6B6UPGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nIniciando carregamento para {final_total_sources} fontes (Web/PDF) - ISSO PODE DEMORAR MUITO...\")\n",
        "# ...(Loading logic using the combined, deduplicated lists remains conceptually the same)...\n",
        "# ...(Includes error handling, temp file logic for PDFs)...\n",
        "all_docs = []\n",
        "loading_errors = []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHViiR_9kuiv",
        "outputId": "221ed005-94d5-4e6d-b3ee-d9a24dae5860"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando carregamento para 108 fontes (Web/PDF) - ISSO PODE DEMORAR MUITO...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Function ---\n",
        "def load_with_error_handling(loader_func, source_identifier):\n",
        "    try:\n",
        "        docs = loader_func()\n",
        "        if not isinstance(docs, list): docs = list(docs)\n",
        "        if docs: print(f\"   + OK ({len(docs)}): {source_identifier}\"); all_docs.extend(docs)\n",
        "        else: print(f\"   - WARN (Empty): {source_identifier}\")\n",
        "        time.sleep(0.3) # Shorter sleep to speed up slightly\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        error_msg = f\"ERROR loading '{source_identifier}': {str(e)[:200]}...\" # Shorter error log\n",
        "        print(f\"   ! {error_msg}\")\n",
        "        loading_errors.append(error_msg)\n",
        "        time.sleep(0.3)\n",
        "        return False"
      ],
      "metadata": {
        "id": "cj8pfXeIP2zW"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Load Web Pages\n",
        "if web_page_urls:\n",
        "    print(f\"\\n-- Loading {len(web_page_urls)} Web Pages...\")\n",
        "    web_start_time = time.time()\n",
        "    web_loader = UnstructuredURLLoader(urls=web_page_urls, ssl_verify=False,\n",
        "                                       headers={'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'},\n",
        "                                       mode=\"elements\", continue_on_failure=True, show_progress_bar=True)\n",
        "    load_with_error_handling(web_loader.load, \"Web Batch\")\n",
        "    print(f\"-- Web loading finished (Time: {time.time() - web_start_time:.2f}s)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdMvlL4lP2qM",
        "outputId": "79f0dc66-cd54-4844-f0af-d8935d81a940"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Loading 57 Web Pages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/57 [00:00<00:10,  5.27it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.googleblog.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'blog.research.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'research.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "  4%|▎         | 2/57 [00:00<00:21,  2.54it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.googleblog.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'blog.research.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'research.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "  5%|▌         | 3/57 [00:01<00:24,  2.19it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.meta.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "  7%|▋         | 4/57 [00:02<00:46,  1.13it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.meta.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "  9%|▉         | 5/57 [00:04<01:10,  1.36s/it]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ai.stanford.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 11%|█         | 6/57 [00:05<00:57,  1.13s/it]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aiindex.stanford.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'hai.stanford.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 12%|█▏        | 7/57 [00:06<00:57,  1.15s/it]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aws.amazon.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 14%|█▍        | 8/57 [00:07<00:44,  1.10it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'aws.amazon.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 16%|█▌        | 9/57 [00:07<00:32,  1.47it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'blog.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 18%|█▊        | 10/57 [00:07<00:26,  1.77it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'blog.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 19%|█▉        | 11/57 [00:08<00:21,  2.12it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'blog.research.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'research.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 21%|██        | 12/57 [00:08<00:23,  1.89it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'blogs.microsoft.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 23%|██▎       | 13/57 [00:08<00:18,  2.43it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'builtin.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'builtin.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 25%|██▍       | 14/57 [00:09<00:16,  2.62it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'colah.github.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 26%|██▋       | 15/57 [00:09<00:12,  3.27it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'cset.georgetown.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 28%|██▊       | 16/57 [00:09<00:16,  2.55it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'deepmind.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 30%|██▉       | 17/57 [00:10<00:14,  2.68it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'deepmind.google'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 32%|███▏      | 18/57 [00:10<00:14,  2.61it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'developer.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'developer.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 35%|███▌      | 20/57 [00:12<00:24,  1.54it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'distill.pub'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 37%|███▋      | 21/57 [00:12<00:20,  1.75it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 39%|███▊      | 22/57 [00:13<00:17,  2.00it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 40%|████      | 23/57 [00:13<00:18,  1.86it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 42%|████▏     | 24/57 [00:14<00:17,  1.84it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 44%|████▍     | 25/57 [00:14<00:16,  1.95it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'futureoflife.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 46%|████▌     | 26/57 [00:17<00:36,  1.17s/it]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'github.blog'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'github.blog'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 47%|████▋     | 27/57 [00:17<00:27,  1.08it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 49%|████▉     | 28/57 [00:18<00:25,  1.13it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 51%|█████     | 29/57 [00:19<00:22,  1.23it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huyenchip.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 53%|█████▎    | 30/57 [00:19<00:18,  1.45it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'jalammar.github.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 54%|█████▍    | 31/57 [00:19<00:13,  1.87it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lilianweng.github.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 56%|█████▌    | 32/57 [00:20<00:13,  1.92it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'lilianweng.github.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 58%|█████▊    | 33/57 [00:20<00:10,  2.33it/s]ERROR:langchain_community.document_loaders.url:Error fetching or processing https://magazine.towardsdatascience.com/attention-is-all-you-need-the-neural-network-that-revolutionised-nlp-60ab88d3e85f, exception: HTTPSConnectionPool(host='magazine.towardsdatascience.com', port=443): Max retries exceeded with url: /attention-is-all-you-need-the-neural-network-that-revolutionised-nlp-60ab88d3e85f (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7ec20e4c5a50>: Failed to resolve 'magazine.towardsdatascience.com' ([Errno -2] Name or service not known)\"))\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openai.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 61%|██████▏   | 35/57 [00:20<00:05,  3.74it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openai.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openai.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 65%|██████▍   | 37/57 [00:20<00:03,  5.48it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openai.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pytorch.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 68%|██████▊   | 39/57 [00:21<00:03,  5.65it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'research.facebook.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 70%|███████   | 40/57 [00:22<00:05,  2.86it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'sebastianraschka.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 72%|███████▏  | 41/57 [00:22<00:04,  3.37it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'spectrum.ieee.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 74%|███████▎  | 42/57 [00:22<00:04,  3.02it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'timdettmers.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 75%|███████▌  | 43/57 [00:23<00:05,  2.79it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'towardsdatascience.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'towardsdatascience.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 77%|███████▋  | 44/57 [00:23<00:04,  2.68it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'towardsdatascience.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'web.stanford.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'hastie.su.domains'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 81%|████████  | 46/57 [00:23<00:02,  3.81it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.assemblyai.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.assemblyai.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 82%|████████▏ | 47/57 [00:24<00:03,  2.66it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.deeplearning.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.eff.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 86%|████████▌ | 49/57 [00:24<00:02,  3.86it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.explainxkcd.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 88%|████████▊ | 50/57 [00:24<00:01,  4.28it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.kdnuggets.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.nature.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 91%|█████████ | 52/57 [00:25<00:01,  3.53it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.nature.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 93%|█████████▎| 53/57 [00:26<00:01,  2.63it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.technologyreview.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 95%|█████████▍| 54/57 [00:26<00:01,  2.48it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.theverge.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.wired.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            " 98%|█████████▊| 56/57 [00:28<00:00,  2.08it/s]/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.wired.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "100%|██████████| 57/57 [00:29<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   + OK (4597): Web Batch\n",
            "-- Web loading finished (Time: 29.31s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Load PDFs\n",
        "if pdf_urls:\n",
        "    print(f\"\\n-- Loading {len(pdf_urls)} PDFs...\")\n",
        "    pdf_start_time = time.time()\n",
        "    for i, url in enumerate(pdf_urls):\n",
        "        temp_pdf_path = None\n",
        "        print(f\"   ({i+1}/{len(pdf_urls)}) PDF: {os.path.basename(urlparse(url).path)}...\") # Show progress\n",
        "        try:\n",
        "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "            response = requests.get(url, stream=True, headers=headers, timeout=120)\n",
        "            response.raise_for_status()\n",
        "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_file:\n",
        "                temp_pdf_path = temp_file.name\n",
        "                for chunk in response.iter_content(chunk_size=1024*1024): temp_file.write(chunk) # 1MB chunks download\n",
        "            pdf_loader_instance = PyPDFLoader(temp_pdf_path, extract_images=False)\n",
        "            def load_pdf_wrapper():\n",
        "                loaded_pdf_docs = pdf_loader_instance.load() # Use basic load, splitting later\n",
        "                for doc in loaded_pdf_docs: doc.metadata['source'] = url\n",
        "                return loaded_pdf_docs\n",
        "            load_with_error_handling(load_pdf_wrapper, url)\n",
        "        except Exception as e_pdf: # Catch all exceptions during PDF handling for this URL\n",
        "            error_msg = f\"ERROR for PDF '{url}': {str(e_pdf)[:200]}...\"\n",
        "            print(f\"   ! {error_msg}\")\n",
        "            loading_errors.append(error_msg)\n",
        "        finally:\n",
        "            if temp_pdf_path and os.path.exists(temp_pdf_path):\n",
        "                try: os.remove(temp_pdf_path)\n",
        "                except OSError as e_rm: print(f\"   ! Warning: Failed removing {temp_pdf_path}: {e_rm}\")\n",
        "            time.sleep(0.5) # Slightly longer sleep between PDFs\n",
        "    print(f\"-- PDF loading finished (Time: {time.time() - pdf_start_time:.2f}s).\")\n",
        "\n",
        "\n",
        "print(f\"\\nTotal documents loaded before splitting: {len(all_docs)}\")\n",
        "if loading_errors:\n",
        "    print(f\"\\nWARNING: Encountered {len(loading_errors)} errors during loading. Knowledge base might be incomplete.\")\n",
        "    # Optionally print unique errors here if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F58vaP-WP2iO",
        "outputId": "91748cd8-01f0-41a7-f729-d280828d99c0"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Loading 51 PDFs...\n",
            "   (1/51) PDF: 1301.3781.pdf...\n",
            "   + OK (12): https://arxiv.org/pdf/1301.3781.pdf\n",
            "   (2/51) PDF: 1312.6114.pdf...\n",
            "   + OK (14): https://arxiv.org/pdf/1312.6114.pdf\n",
            "   (3/51) PDF: 1406.2661.pdf...\n",
            "   + OK (9): https://arxiv.org/pdf/1406.2661.pdf\n",
            "   (4/51) PDF: 1409.0473.pdf...\n",
            "   + OK (15): https://arxiv.org/pdf/1409.0473.pdf\n",
            "   (5/51) PDF: 1409.3215.pdf...\n",
            "   + OK (9): https://arxiv.org/pdf/1409.3215.pdf\n",
            "   (6/51) PDF: 1412.6980.pdf...\n",
            "   + OK (15): https://arxiv.org/pdf/1412.6980.pdf\n",
            "   (7/51) PDF: 1502.01852.pdf...\n",
            "   + OK (11): https://arxiv.org/pdf/1502.01852.pdf\n",
            "   (8/51) PDF: 1506.02640.pdf...\n",
            "   + OK (10): https://arxiv.org/pdf/1506.02640.pdf\n",
            "   (9/51) PDF: 1511.06434.pdf...\n",
            "   + OK (16): https://arxiv.org/pdf/1511.06434.pdf\n",
            "   (10/51) PDF: 1512.03385.pdf...\n",
            "   + OK (12): https://arxiv.org/pdf/1512.03385.pdf\n",
            "   (11/51) PDF: 1603.05027.pdf...\n",
            "   + OK (15): https://arxiv.org/pdf/1603.05027.pdf\n",
            "   (12/51) PDF: 1703.06870.pdf...\n",
            "   + OK (12): https://arxiv.org/pdf/1703.06870.pdf\n",
            "   (13/51) PDF: 1706.03762.pdf...\n",
            "   + OK (15): https://arxiv.org/pdf/1706.03762.pdf\n",
            "   (14/51) PDF: 1707.06347.pdf...\n",
            "   + OK (12): https://arxiv.org/pdf/1707.06347.pdf\n",
            "   (15/51) PDF: 1710.10903.pdf...\n",
            "   + OK (12): https://arxiv.org/pdf/1710.10903.pdf\n",
            "   (16/51) PDF: 1810.04805.pdf...\n",
            "   + OK (16): https://arxiv.org/pdf/1810.04805.pdf\n",
            "   (17/51) PDF: 1907.11692.pdf...\n",
            "   + OK (13): https://arxiv.org/pdf/1907.11692.pdf\n",
            "   (18/51) PDF: 1909.11553.pdf...\n",
            "   + OK (11): https://arxiv.org/pdf/1909.11553.pdf\n",
            "   (19/51) PDF: 1911.11427.pdf...\n",
            "   + OK (16): https://arxiv.org/pdf/1911.11427.pdf\n",
            "   (20/51) PDF: 2005.14165.pdf...\n",
            "   + OK (75): https://arxiv.org/pdf/2005.14165.pdf\n",
            "   (21/51) PDF: 2006.11239.pdf...\n",
            "   + OK (25): https://arxiv.org/pdf/2006.11239.pdf\n",
            "   (22/51) PDF: 2010.11929.pdf...\n",
            "   + OK (22): https://arxiv.org/pdf/2010.11929.pdf\n",
            "   (23/51) PDF: 2103.00020.pdf...\n",
            "   + OK (48): https://arxiv.org/pdf/2103.00020.pdf\n",
            "   (24/51) PDF: 2104.09755.pdf...\n",
            "   + OK (15): https://arxiv.org/pdf/2104.09755.pdf\n",
            "   (25/51) PDF: 2106.09685.pdf...\n",
            "   + OK (26): https://arxiv.org/pdf/2106.09685.pdf\n",
            "   (26/51) PDF: 2112.10752.pdf...\n",
            "   + OK (45): https://arxiv.org/pdf/2112.10752.pdf\n",
            "   (27/51) PDF: 2203.02155.pdf...\n",
            "   + OK (68): https://arxiv.org/pdf/2203.02155.pdf\n",
            "   (28/51) PDF: 2204.02311.pdf...\n",
            "   + OK (87): https://arxiv.org/pdf/2204.02311.pdf\n",
            "   (29/51) PDF: 2205.11487.pdf...\n",
            "   + OK (46): https://arxiv.org/pdf/2205.11487.pdf\n",
            "   (30/51) PDF: 2302.13971.pdf...\n",
            "   + OK (27): https://arxiv.org/pdf/2302.13971.pdf\n",
            "   (31/51) PDF: 2303.10130.pdf...\n",
            "   + OK (36): https://arxiv.org/pdf/2303.10130.pdf\n",
            "   (32/51) PDF: 2304.07193.pdf...\n",
            "   + OK (32): https://arxiv.org/pdf/2304.07193.pdf\n",
            "   (33/51) PDF: 2304.08485.pdf...\n",
            "   + OK (25): https://arxiv.org/pdf/2304.08485.pdf\n",
            "   (34/51) PDF: 2305.11206.pdf...\n",
            "   + OK (15): https://arxiv.org/pdf/2305.11206.pdf\n",
            "   (35/51) PDF: 2305.18290.pdf...\n",
            "   + OK (27): https://arxiv.org/pdf/2305.18290.pdf\n",
            "   (36/51) PDF: 2306.06174.pdf...\n",
            "   + OK (31): https://arxiv.org/pdf/2306.06174.pdf\n",
            "   (37/51) PDF: 2306.15595.pdf...\n",
            "   + OK (18): https://arxiv.org/pdf/2306.15595.pdf\n",
            "   (38/51) PDF: 2307.09288.pdf...\n",
            "   + OK (77): https://arxiv.org/pdf/2307.09288.pdf\n",
            "   (39/51) PDF: 2308.10383.pdf...\n",
            "   + OK (16): https://arxiv.org/pdf/2308.10383.pdf\n",
            "   (40/51) PDF: 2310.13592.pdf...\n",
            "   + OK (16): https://arxiv.org/pdf/2310.13592.pdf\n",
            "   (41/51) PDF: 2311.10122.pdf...\n",
            "   + OK (14): https://arxiv.org/pdf/2311.10122.pdf\n",
            "   (42/51) PDF: 2401.04088.pdf...\n",
            "   + OK (13): https://arxiv.org/pdf/2401.04088.pdf\n",
            "   (43/51) PDF: 2402.17764.pdf...\n",
            "   + OK (8): https://arxiv.org/pdf/2402.17764.pdf\n",
            "   (44/51) PDF: 2404.11225...\n",
            "   + OK (18): https://arxiv.org/pdf/2404.11225\n",
            "   (45/51) PDF: 2404.14219...\n",
            "   + OK (24): https://arxiv.org/pdf/2404.14219\n",
            "   (46/51) PDF: 9704103.pdf...\n",
            "   ! ERROR for PDF 'https://arxiv.org/pdf/cs/9704103.pdf': 404 Client Error: Not Found for url: http://arxiv.org/pdf/cs/9704103...\n",
            "   (47/51) PDF: gpt-4o-system-card.pdf...\n",
            "   + OK (32): https://cdn.openai.com/gpt-4o-system-card.pdf\n",
            "   (48/51) PDF: glorot10a.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 2 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 34 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 92 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 145 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 206 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 274 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 330 65536 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 372 65536 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   + OK (8): https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
            "   (49/51) PDF: c399862d3b9d6b76c8436e924a68c45b-Paper.pdf...\n",
            "   + OK (9): https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
            "   (50/51) PDF: 45530.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf.generic._base:could not convert string to float: b'0.00-34120734' : FloatObject (b'0.00-34120734') invalid; use 0.0 instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   + OK (8): https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf\n",
            "   (51/51) PDF: gemma-report.pdf...\n",
            "   + OK (17): https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf\n",
            "-- PDF loading finished (Time: 109.35s).\n",
            "\n",
            "Total documents loaded before splitting: 5770\n",
            "\n",
            "WARNING: Encountered 1 errors during loading. Knowledge base might be incomplete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 4. Processar Dados (Split) ===\n",
        " - ...(Splitting logic remains similar, check params)"
      ],
      "metadata": {
        "id": "MWzmwwQEX2sN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#...(Splitting logic remains similar, check params)\n",
        "all_splits = []\n",
        "if all_docs:\n",
        "    print(f\"\\nSplitting {len(all_docs)} documents into chunks...\")\n",
        "    # Larger overlap might help with context across ~1k chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000, chunk_overlap=250, add_start_index=True,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \";\", \",\", \" \", \"\"], keep_separator=False\n",
        "    )\n",
        "    try:\n",
        "      start_split_time = time.time()\n",
        "      all_splits = text_splitter.split_documents(all_docs)\n",
        "      split_duration = time.time() - start_split_time\n",
        "      print(f\"Splitting complete: {len(all_splits)} chunks created (Time: {split_duration:.2f}s).\")\n",
        "      if not all_splits and all_docs: print(\"WARNING: Splitting yielded 0 chunks.\")\n",
        "    except Exception as split_e: print(f\"CRITICAL Error during splitting: {split_e}\"); all_splits = []\n",
        "else: print(\"No documents loaded to split.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib0p-FR3Xvm5",
        "outputId": "2d53c4f7-c8ca-4f31-9e8c-a21f8109f7f7"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Splitting 5770 documents into chunks...\n",
            "Splitting complete: 9687 chunks created (Time: 0.79s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 5. Embeddings & Vector Store ==="
      ],
      "metadata": {
        "id": "XcGgbOKKb9U5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = None\n",
        "# !! NOVO NOME DE ÍNDICE PARA O DATASET EXPANDIDO SEM YT!!\n",
        "FAISS_INDEX_PATH = \"faiss_index_ai_history_expanded_ada_v1\"\n",
        "# !! TRUE para a PRIMEIRA execução desta versão!!\n",
        "FORCE_RECREATE_INDEX = True\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\" # Still using ada-002\n",
        "\n",
        "if all_splits:\n",
        "    print(f\"\\nConfiguring FAISS (Path: '{FAISS_INDEX_PATH}', Recreate: {FORCE_RECREATE_INDEX})...\")\n",
        "    try:\n",
        "        embeddings = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
        "        print(f\"Using embedding model: {embeddings.model}\")\n",
        "\n",
        "        if os.path.exists(FAISS_INDEX_PATH) and not FORCE_RECREATE_INDEX:\n",
        "            print(f\"-- Loading existing FAISS index for {embeddings.model} from '{FAISS_INDEX_PATH}'...\")\n",
        "            # Load logic... (as before)\n",
        "            vector_store = FAISS.load_local(FAISS_INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
        "            print(\"-- Index loaded.\")\n",
        "        else:\n",
        "             if os.path.exists(FAISS_INDEX_PATH):\n",
        "                 print(f\"-- Removing old/incompatible FAISS index: '{FAISS_INDEX_PATH}'...\")\n",
        "                 try: shutil.rmtree(FAISS_INDEX_PATH); print(\"-- Old index removed.\")\n",
        "                 except OSError as e_rm: print(f\"-- Warning: Failed removing old index: {e_rm}.\")\n",
        "\n",
        "             # Create - THIS WILL TAKE A LONG TIME AND USE SIGNIFICANT API CREDITS\n",
        "             print(f\"-- Creating NEW FAISS index from {len(all_splits)} chunks using {embeddings.model}...\")\n",
        "             print(\"-- This involves sending data to OpenAI for embeddings and can take time/cost credits --\")\n",
        "             start_create_time = time.time()\n",
        "             vector_store = FAISS.from_documents(all_splits, embeddings)\n",
        "             create_duration = time.time() - start_create_time\n",
        "             print(f\"-- Index created (Time: {create_duration:.2f}s).\")\n",
        "\n",
        "             # Save\n",
        "             print(f\"-- Saving FAISS index to '{FAISS_INDEX_PATH}'...\")\n",
        "             #...(Save logic as before)\n",
        "             if not os.path.exists(FAISS_INDEX_PATH): os.makedirs(FAISS_INDEX_PATH)\n",
        "             vector_store.save_local(FAISS_INDEX_PATH)\n",
        "             print(\"-- Index saved.\")\n",
        "\n",
        "    except Exception as e_faiss: print(f\"CRITICAL FAISS Error ({embeddings.model}): {e_faiss}\"); print(traceback.format_exc()[:1000]); vector_store = None\n",
        "else: print(\"No chunks to process. FAISS store not configured.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkxqXC8gb4OQ",
        "outputId": "acd5e7e7-266d-43e5-b0dc-939ad3b6198d"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Configuring FAISS (Path: 'faiss_index_ai_history_expanded_ada_v1', Recreate: True)...\n",
            "Using embedding model: text-embedding-ada-002\n",
            "-- Removing old/incompatible FAISS index: 'faiss_index_ai_history_expanded_ada_v1'...\n",
            "-- Old index removed.\n",
            "-- Creating NEW FAISS index from 9687 chunks using text-embedding-ada-002...\n",
            "-- This involves sending data to OpenAI for embeddings and can take time/cost credits --\n",
            "-- Index created (Time: 41.67s).\n",
            "-- Saving FAISS index to 'faiss_index_ai_history_expanded_ada_v1'...\n",
            "-- Index saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 6. Definir a Cadeia RAG (LangGraph) ===\n",
        "- ...(RAG Graph setup mostly the same, but update prompt and consider retriever K)..."
      ],
      "metadata": {
        "id": "0ZNGGE6BewE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDefining RAG graph...\")\n",
        "prompt = None; rag_graph = None\n",
        "\n",
        "if vector_store and llm:\n",
        "    try:\n",
        "        prompt = hub.pull(\"rlm/rag-prompt\") # Base prompt\n",
        "        # UPDATED PROMPT FOR EXPANDED SCOPE\n",
        "        prompt.messages[0].prompt.template = \"\"\"Aja com um assistente de IA especialista em história da Inteligência Artificial.\n",
        "        Você tem profundo conhecimento dos principais avanços, marcos, paradigmas e transformações ocorridas na IA desde 1990 até o presente.\n",
        "        Seu treinamento é baseado exclusivamente em artigos da web e documentos PDF, especialmente aquele fornecido nesta conversa.\n",
        "        Seu objetivo é responder a perguntas específicas sobre a história e os avanços da IA de forma factual, clara e concisa.\n",
        "        Você deve usar estritamente o conteúdo do(s) documento(s) fornecido(s) como referência para suas respostas.\n",
        "        Instruções passo a passo:\n",
        "        Leia a pergunta feita pelo usuário.\n",
        "        Reflita se há uma resposta direta, clara e verificável dentro do(s) documento(s) fornecido(s).\n",
        "        Se houver uma resposta:\n",
        "        Escreva de forma clara e concisa (em 4 a 6 frases no máximo).\n",
        "        Separe os parágrafos para facilitar a leitura.\n",
        "        Limite-se ao contexto fornecido.\n",
        "        Se não houver resposta ou evidência suficiente no conteúdo fornecido:\n",
        "        Responda exatamente com: \"Com base nas informações disponíveis, não sei responder a essa pergunta.\"\n",
        "        Nunca utilize conhecimento externo, mesmo que você saiba a resposta.\n",
        "        Responda em Português.\n",
        "\n",
        "Contexto Fornecido:\n",
        "{context}\n",
        "\n",
        "Pergunta do Usuário: {question}\n",
        "\n",
        "Resposta Factual e Concisa:\"\"\"\n",
        "        print(\"RAG prompt adjusted for WIDE historical scope & Portuguese.\")\n",
        "\n",
        "        class State(TypedDict): question: str; context: List[Document]; answer: str\n",
        "\n",
        "        def retrieve(state: State):\n",
        "            question = state[\"question\"]\n",
        "            # INCREASE K further due to very large index? Test this value.\n",
        "            k_results = 10 # Increased k\n",
        "            relevance_threshold = 0.70 # Keep threshold, maybe adjust later\n",
        "            print(f\"-- Retrieving context (k={k_results}, thr={relevance_threshold}) via FAISS for: '{question}' --\")\n",
        "            try:\n",
        "                 docs_scores = vector_store.similarity_search_with_relevance_scores(question, k=k_results)\n",
        "                 filtered_docs = [doc for doc, score in docs_scores if score >= relevance_threshold]\n",
        "                 print(f\"-- Retrieved {len(docs_scores)} chunks, {len(filtered_docs)} relevant.\")\n",
        "                 if not filtered_docs and docs_scores: # Fallback\n",
        "                     filtered_docs = [doc for doc, score in docs_scores[:min(3, len(docs_scores))]] # Keep top 3 if none pass threshold\n",
        "                     print(f\"-- Threshold not met, using top {len(filtered_docs)} chunks instead.\")\n",
        "                 return {\"context\": filtered_docs, \"question\": question}\n",
        "            except Exception as retrieve_e: print(f\"! FAISS Search Error: {retrieve_e}\"); return {\"context\": [], \"question\": question}\n",
        "\n",
        "        def generate(state: State):\n",
        "             #...(Generate logic mostly same, ensure formatting works)...\n",
        "             print(\"-- Generating answer based on context --\")\n",
        "             question = state[\"question\"]; context = state[\"context\"]\n",
        "             if not context: return {\"answer\": \"Com base nas informações disponíveis, não sei responder a essa pergunta.\"}\n",
        "             docs_content_list = []\n",
        "             for i, doc in enumerate(context):\n",
        "                  source=doc.metadata.get('source','?'); page=doc.metadata.get('page','');\n",
        "                  s_display = f\"Fonte {i+1}: {os.path.basename(urlparse(source).path)}\" # Shorter display name\n",
        "                  if page!='': s_display+=f\" (P.{page+1})\"\n",
        "                  docs_content_list.append(f\"{s_display}\\n{doc.page_content}\")\n",
        "             docs_content = \"\\n\\n---\\n\".join(docs_content_list)\n",
        "             try:\n",
        "                 messages = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
        "                 response = llm.invoke(messages); print(\"-- LLM response received --\")\n",
        "                 return {\"answer\": response.content.strip()}\n",
        "             except Exception as invoke_e: print(f\"! LLM Error: {invoke_e}\"); return {\"answer\": f\"Error generating response: {type(invoke_e).__name__}\"}\n",
        "\n",
        "        # Build Graph (same structure)\n",
        "        graph_builder = StateGraph(State); graph_builder.add_node(\"retrieve\", retrieve); graph_builder.add_node(\"generate\", generate)\n",
        "        graph_builder.set_entry_point(\"retrieve\"); graph_builder.add_edge(\"retrieve\", \"generate\"); graph_builder.set_finish_point(\"generate\")\n",
        "        rag_graph = graph_builder.compile()\n",
        "        print(\"RAG graph compiled successfully.\")\n",
        "\n",
        "    except Exception as graph_e: print(f\"CRITICAL Error building RAG graph: {graph_e}\"); print(traceback.format_exc()[:1000]); rag_graph = None\n",
        "else: print(\"Could not define RAG graph (Missing LLM or Vector Store).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5yWMIooeyWA",
        "outputId": "dedeec4e-889e-4c93-f057-a559c1c34c7d"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defining RAG graph...\n",
            "RAG prompt adjusted for WIDE historical scope & Portuguese.\n",
            "RAG graph compiled successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 7. Consultar o Assistente ==="
      ],
      "metadata": {
        "id": "Hben8XGUe-63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if rag_graph:\n",
        "    print(f\"\\n=== RAG Assistant (EXPANDED Web/PDFs, History 1990s-Present, Embed: {EMBEDDING_MODEL_NAME}) Ready ===\")\n",
        "    #...(Query loop remains the same)...\n",
        "    print(f\"Using FAISS index from: '{FAISS_INDEX_PATH}'\")\n",
        "    print(\"Ask questions about AI history/concepts/models. Type 'sair' to exit.\")\n",
        "    while True:\n",
        "        try:\n",
        "            user_question = input(\"\\nYour question: \")\n",
        "            if user_question.lower() == 'sair': break\n",
        "            if not user_question.strip(): continue\n",
        "            start_query_time = time.time()\n",
        "            final_state = rag_graph.invoke({\"question\": user_question})\n",
        "            query_duration = time.time() - start_query_time\n",
        "            print(f\"\\nAssistant's Answer: (Replied in {query_duration:.2f}s)\")\n",
        "            print(final_state.get('answer', \"Error: No answer found.\"))\n",
        "        except KeyboardInterrupt: print(\"\\nExiting...\"); break\n",
        "        except Exception as loop_e: print(f\"\\nUnexpected error in query loop: {loop_e}\")\n",
        "    print(\"\\nAssistant terminated.\")\n",
        "else:\n",
        "    print(\"\\nRAG assistant could not be started due to critical setup errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzVsfhXUe3Ue",
        "outputId": "b6d5ffb6-ac16-4adf-feb2-ebb710e21c1d"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== RAG Assistant (EXPANDED Web/PDFs, History 1990s-Present, Embed: text-embedding-ada-002) Ready ===\n",
            "Using FAISS index from: 'faiss_index_ai_history_expanded_ada_v1'\n",
            "Ask questions about AI history/concepts/models. Type 'sair' to exit.\n",
            "\n",
            "Your question: Compare as abordagens de pré-treinamento do BERT e do GPT original.\n",
            "-- Retrieving context (k=10, thr=0.7) via FAISS for: 'Compare as abordagens de pré-treinamento do BERT e do GPT original.' --\n",
            "-- Retrieved 10 chunks, 10 relevant.\n",
            "-- Generating answer based on context --\n",
            "-- LLM response received --\n",
            "\n",
            "Assistant's Answer: (Replied in 6.09s)\n",
            "As abordagens de pré-treinamento do BERT e do GPT original diferem significativamente em sua arquitetura e metodologia. O GPT utiliza um modelo de Transformer unidirecional, treinando de forma sequencial da esquerda para a direita, enquanto o BERT adota um modelo de Transformer bidirecional, permitindo que as representações sejam condicionadas tanto ao contexto à esquerda quanto à direita em todas as camadas.\n",
            "\n",
            "Além disso, o GPT é treinado em um corpus de texto que inclui o BooksCorpus, enquanto o BERT é treinado em uma combinação do BooksCorpus e da Wikipedia, resultando em um conjunto de dados maior. O BERT também implementa duas tarefas de pré-treinamento, que são fundamentais para suas melhorias empíricas, ao contrário do GPT, que tem um único objetivo de treinamento. Essas diferenças estruturais e de treinamento são cruciais para o desempenho distinto de cada modelo em tarefas de processamento de linguagem natural.\n",
            "\n",
            "Your question: sair\n",
            "\n",
            "Assistant terminated.\n"
          ]
        }
      ]
    }
  ]
}